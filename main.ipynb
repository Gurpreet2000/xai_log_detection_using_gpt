{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read raw log entries from file\n",
    "with open(\"./dataset/HDFS_v1/HDFS.log\", \"r\") as f:\n",
    "    raw_logs = f.readlines()\n",
    "# Optionally, clean (strip whitespace) and filter empty lines\n",
    "raw_logs = [log.strip() for log in raw_logs if log.strip()]\n",
    "print(\"Number of raw logs:\", len(raw_logs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structure data using Drain3\n",
    "\n",
    "from drain3.template_miner_config import TemplateMinerConfig\n",
    "from drain3.template_miner import TemplateMiner\n",
    "\n",
    "# Configure Drain (adjust parameters as needed)\n",
    "config = TemplateMinerConfig()\n",
    "config.sim_th = 0.4\n",
    "config.max_depth = 4\n",
    "\n",
    "# Initialize the TemplateMiner by specifying keyword arguments\n",
    "miner = TemplateMiner(persistence_handler=None, config=config)\n",
    "\n",
    "parsed_logs = []\n",
    "for log in raw_logs:  # ensure raw_logs is defined\n",
    "    result = miner.add_log_message(log)\n",
    "    # Each result has a template ID and the template string\n",
    "    parsed_template = result[\"template_mined\"]\n",
    "    parsed_logs.append(parsed_template)\n",
    "\n",
    "# Show a sample parsed log\n",
    "print(\"Sample parsed log:\", parsed_logs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization with GPTâ€‘2 Tokenizer\n",
    "\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Tokenize each parsed log (adding special tokens)\n",
    "tokenized_logs = [tokenizer.encode(log, add_special_tokens=True) for log in parsed_logs]\n",
    "print(\"Token lengths:\", [len(t) for t in tokenized_logs if t])  # only non-empty lists\n",
    "\n",
    "# Filter out any empty tokenizations\n",
    "tokenized_logs = [t for t in tokenized_logs if len(t) > 0]\n",
    "\n",
    "print(\"Example tokenized log:\", tokenized_logs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a PyTorch Dataset for Fine-Tuning\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class LogDataset(Dataset):\n",
    "    def __init__(self, logs, tokenizer, block_size=128):\n",
    "        # Ensure logs are concatenated and tokenized\n",
    "        text = \" \".join(logs)\n",
    "        \n",
    "        # Tokenize the entire text\n",
    "        tokenized_text = tokenizer.encode(text, add_special_tokens=True)\n",
    "        \n",
    "        # Create blocks\n",
    "        self.examples = []\n",
    "        for i in range(0, len(tokenized_text) - block_size + 1, block_size):\n",
    "            block = tokenized_text[i:i + block_size]\n",
    "            \n",
    "            # Pad if necessary\n",
    "            if len(block) < block_size:\n",
    "                block = block + [tokenizer.pad_token_id] * (block_size - len(block))\n",
    "            \n",
    "            # Convert to tensor\n",
    "            block_tensor = torch.tensor(block, dtype=torch.long)\n",
    "            self.examples.append({\n",
    "                \"input_ids\": block_tensor,\n",
    "                \"labels\": block_tensor,\n",
    "                \"attention_mask\": torch.ones_like(block_tensor)\n",
    "            })\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.examples[idx]\n",
    "\n",
    "# Create the dataset\n",
    "log_dataset = LogDataset(raw_logs, tokenizer, block_size=128)\n",
    "print(\"Number of training sequences:\", len(log_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-Tuning GPTâ€‘2 on Normal Logs\n",
    "from transformers import GPT2LMHeadModel, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "\n",
    "import os\n",
    "\n",
    "def get_latest_checkpoint(output_dir):\n",
    "    checkpoints = [\n",
    "        os.path.join(output_dir, d)\n",
    "        for d in os.listdir(output_dir)\n",
    "        if d.startswith(\"checkpoint-\") and os.path.isdir(os.path.join(output_dir, d))\n",
    "    ]\n",
    "    if checkpoints:\n",
    "        # Return the checkpoint with the latest modification time\n",
    "        return max(checkpoints, key=os.path.getmtime)\n",
    "    return None\n",
    "\n",
    "output_dir = \"./gpt2_log_finetuned\"\n",
    "latest_checkpoint = get_latest_checkpoint(output_dir)\n",
    "\n",
    "# Step 1: Load model\n",
    "print(\"ðŸ” Loading GPT-2 model...\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\", output_attentions=True)\n",
    "model.tie_weights()\n",
    "# Run on GPU if available\n",
    "model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Ensure padding token is set\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.generation_config.return_dict_in_generate = True\n",
    "print(\"âœ… Model loaded successfully.\")\n",
    "\n",
    "# Step 2: Configure training\n",
    "print(\"ðŸ›  Configuring training arguments...\")\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    prediction_loss_only=True,\n",
    "    logging_steps=10,\n",
    "    logging_dir=\"./logs\",\n",
    "    resume_from_checkpoint=latest_checkpoint\n",
    ")\n",
    "print(\"âœ… Training arguments set.\")\n",
    "\n",
    "# Step 3: Create data collator\n",
    "print(\"ðŸ“¦ Creating data collator...\")\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,  # GPTâ€‘2 uses causal language modeling\n",
    ")\n",
    "print(\"âœ… Data collator ready.\")\n",
    "\n",
    "# Step 4: Inspect dataset briefly\n",
    "print(f\"ðŸ“Š Total training sequences: {len(log_dataset)}\")\n",
    "print(\"ðŸ“Œ Example input_ids from dataset:\", log_dataset[0][\"input_ids\"][:10].tolist())\n",
    "print(\"ðŸ“Œ Decoded example:\", tokenizer.decode(log_dataset[0][\"input_ids\"]))\n",
    "\n",
    "# Step 5: Initialize Trainer\n",
    "print(\"ðŸš€ Initializing Trainer...\")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=log_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "print(\"âœ… Trainer initialized.\")\n",
    "\n",
    "# Sanity check: Try a single forward pass\n",
    "sample = log_dataset[0]\n",
    "device = model.device  # Automatically get GPU if model is on CUDA\n",
    "\n",
    "inputs = sample[\"input_ids\"].unsqueeze(0).to(device)  # Move to same device\n",
    "outputs = model(input_ids=inputs, labels=inputs)\n",
    "print(\"âœ… Forward pass successful. Loss:\", outputs.loss.item())\n",
    "\n",
    "\n",
    "# Step 6: Start fine-tuning\n",
    "print(\"ðŸŽ¯ Starting fine-tuning...\")\n",
    "trainer.train(resume_from_checkpoint=latest_checkpoint if True else None)\n",
    "print(\"ðŸ Fine-tuning complete.\")\n",
    "\n",
    "# Step 7: Save model and tokenizer\n",
    "print(\"ðŸ’¾ Saving model and tokenizer...\")\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "print(f\"âœ… Model and tokenizer saved at {output_dir}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract loss values and corresponding training steps from trainer.state.log_history\n",
    "loss_history = [entry[\"loss\"] for entry in trainer.state.log_history if \"loss\" in entry]\n",
    "steps = [entry[\"step\"] for entry in trainer.state.log_history if \"loss\" in entry]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(steps, loss_history, marker=\"o\", linestyle=\"--\")\n",
    "plt.xlabel(\"Training Steps\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss vs. Steps\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "def compute_perplexity(model, tokenizer, log_line):\n",
    "    # Encode the log line into tokens and move to model's device\n",
    "    inputs = tokenizer.encode(log_line, return_tensors=\"pt\").to(model.device)\n",
    "    # Forward pass with labels (language modeling loss)\n",
    "    outputs = model(inputs, labels=inputs)\n",
    "    loss = outputs.loss\n",
    "    perplexity = math.exp(loss.item())\n",
    "    return perplexity\n",
    "\n",
    "# Test with a sample log (using one of the parsed logs as example)\n",
    "sample_log = parsed_logs[0]\n",
    "pp = compute_perplexity(model, tokenizer, sample_log)\n",
    "print(f\"Perplexity for sample log: {pp:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens, importance = extract_attention(model, tokenizer, sample_log)\n",
    "for token, imp in zip(tokens, importance.tolist()):\n",
    "    print(f\"{token}: {imp:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generate_explanation(model, tokenizer, log_line, top_k=3):\n",
    "    \"\"\"\n",
    "    Generate a textual explanation for a log entry based on its attention scores.\n",
    "    This function extracts tokens, normalizes their attention scores, and\n",
    "    identifies the top-k tokens that most influenced the prediction.\n",
    "    \"\"\"\n",
    "    tokens, importance = extract_attention(model, tokenizer, log_line)\n",
    "    \n",
    "    # Normalize attention scores so that they sum to 1 (for easier interpretation)\n",
    "    norm_importance = importance / torch.sum(importance)\n",
    "    \n",
    "    # Retrieve the top-k tokens with the highest normalized attention scores\n",
    "    top_values, top_indices = torch.topk(norm_importance, top_k)\n",
    "    \n",
    "    # Create a list of tuples (token, normalized score)\n",
    "    explanation_tokens = [(tokens[i], top_values[i].item()) for i in top_indices]\n",
    "    \n",
    "    # Create a narrative explanation\n",
    "    explanation_msg = (\n",
    "        \"The log entry was flagged as anomalous because the following tokens \"\n",
    "        \"received the highest attention: \" +\n",
    "        \", \".join([f\"'{tok}' (score: {score:.2f})\" for tok, score in explanation_tokens]) +\n",
    "        \".\"\n",
    "    )\n",
    "    return explanation_msg, explanation_tokens\n",
    "\n",
    "def visualize_attention(tokens, importance):\n",
    "    \"\"\"\n",
    "    Visualize the attention scores as a bar chart.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 2))\n",
    "    plt.bar(range(len(tokens)), importance.tolist(), tick_label=tokens)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.xlabel(\"Tokens\")\n",
    "    plt.ylabel(\"Attention Score\")\n",
    "    plt.title(\"Token Attention Weights\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "# Assume `model` and `tokenizer` are already loaded (e.g., GPT2LMHeadModel and GPT2Tokenizer),\n",
    "# and `sample_log` is one of your parsed log lines.\n",
    "sample_log = \"ERROR: HDFC_v1 connection timeout after 500ms\"  # Replace with your actual log string\n",
    "\n",
    "# Generate explanation for the sample log entry\n",
    "explanation, top_tokens = generate_explanation(model, tokenizer, sample_log, top_k=3)\n",
    "print(\"Explanation:\", explanation)\n",
    "print(\"Top tokens with attention scores:\", top_tokens)\n",
    "\n",
    "# Optionally, visualize the attention for the entire log line\n",
    "tokens, importance = extract_attention(model, tokenizer, sample_log)\n",
    "visualize_attention(tokens, importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_anomaly(log_line, threshold=50):\n",
    "    perplexity = compute_perplexity(model, tokenizer, log_line)\n",
    "    is_anomaly = perplexity > threshold\n",
    "    return is_anomaly, perplexity\n",
    "\n",
    "# Example usage:\n",
    "is_anomaly, perplexity = detect_anomaly(sample_log, threshold=50)\n",
    "print(\"Is Anomalous:\", is_anomaly, \"Perplexity:\", perplexity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_attention(model, tokenizer, log_line):\n",
    "    \"\"\"\n",
    "    Tokenize the input log line, run it through the model with attention output,\n",
    "    and return the tokens along with aggregated attention scores.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer.encode(log_line, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model(inputs, output_attentions=True)\n",
    "    \n",
    "    # Stack and average attention across layers and heads\n",
    "    all_attentions = torch.stack(outputs.attentions)  # (num_layers, batch, num_heads, seq_len, seq_len)\n",
    "    avg_attention = torch.mean(all_attentions, dim=(0, 2))  # (batch, seq_len, seq_len)\n",
    "    \n",
    "    # Sum attention for each token over all positions\n",
    "    token_importance = torch.sum(avg_attention[0], dim=0)\n",
    "    \n",
    "    # Convert token IDs back to text tokens\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs[0])\n",
    "    return tokens, token_importance\n",
    "\n",
    "def generate_explanation(model, tokenizer, log_line, top_k=3):\n",
    "    \"\"\"\n",
    "    Generate a textual explanation for a log entry based on its attention scores.\n",
    "    \"\"\"\n",
    "    tokens, importance = extract_attention(model, tokenizer, log_line)\n",
    "    norm_importance = importance / torch.sum(importance)  # Normalize scores\n",
    "    \n",
    "    # Retrieve the top-k tokens with highest normalized attention scores\n",
    "    top_values, top_indices = torch.topk(norm_importance, top_k)\n",
    "    explanation_tokens = [(tokens[i], top_values[i].item()) for i in top_indices]\n",
    "    \n",
    "    explanation_msg = (\n",
    "        \"The log entry was flagged as anomalous because the following tokens \"\n",
    "        \"received the highest attention: \" +\n",
    "        \", \".join([f\"'{tok}' (score: {score:.2f})\" for tok, score in explanation_tokens]) +\n",
    "        \".\"\n",
    "    )\n",
    "    return explanation_msg, explanation_tokens\n",
    "\n",
    "# Example usage:\n",
    "explanation, top_tokens = generate_explanation(model, tokenizer, sample_log, top_k=3)\n",
    "print(\"Explanation:\", explanation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_log_entry(log_line, threshold=50, top_k=3):\n",
    "    # Step 1: Compute perplexity\n",
    "    is_anomaly, perplexity = detect_anomaly(log_line, threshold)\n",
    "    \n",
    "    # Step 2: Generate explanation if anomaly is detected\n",
    "    if is_anomaly:\n",
    "        explanation, tokens_info = generate_explanation(model, tokenizer, log_line, top_k)\n",
    "    else:\n",
    "        explanation = \"Log entry is normal.\"\n",
    "        tokens_info = []\n",
    "    \n",
    "    return {\n",
    "        \"log_line\": log_line,\n",
    "        \"perplexity\": perplexity,\n",
    "        \"is_anomaly\": is_anomaly,\n",
    "        \"explanation\": explanation,\n",
    "        \"top_tokens\": tokens_info\n",
    "    }\n",
    "\n",
    "# Process a sample log entry\n",
    "result = process_log_entry(sample_log)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute token lengths for each tokenized log entry\n",
    "token_lengths = [len(t) for t in tokenized_logs]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(token_lengths, bins=20, edgecolor=\"black\")\n",
    "plt.xlabel(\"Token Length\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Token Length Distribution\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Compute perplexity for each log entry in raw_logs\n",
    "perplexities = [compute_perplexity(model, tokenizer, log) for log in raw_logs]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(perplexities, bins=20, edgecolor=\"black\")\n",
    "plt.xlabel(\"Perplexity\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Perplexity Distribution Across Log Lines\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
